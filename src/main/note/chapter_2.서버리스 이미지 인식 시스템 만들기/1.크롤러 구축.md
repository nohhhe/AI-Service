1.크롤러 구축
=========================
## ⭕ 아키텍처
<img src="../../resources/images/크롤러 구축 아키텍처.jpg" alt="크롤러 구축 아키텍처" style="width: 100%; height: auto;" />

- S3에 업로드 된 프런트엔드에서 API 게이트 웨이를 통해 API 서버를 호출한다.
- SQS 메시지가 비동기식 람다 함수를 트리거한다.
- API 게이트웨이에서 오는 이벤트가 동기식 람다 함수를 트리거된다.
- 레코그니션은 완전 관리형 AI 이미지 분석 서비스이다.

### ◉ 웹 애플리케이션
프런트엔드는 HTML과 CSS, UI를 렌더링하기 위한 간단한 자바스크립트로 구성된 단일 페이지 애플리케이션이다.
프런트엔드는 S3 버킷에 배포된다. 이때 API 게이트웨이를 사용하여 프런트엔드에 랜더링 데이터를 제공하는 동기식 서비스에 대한 경로를 제공한다.

### ◉ 동기식 서비스
API 게이트웨이를 통해 액세스되는 REST API의 엔드포인트로 사용한다.
- POST /url/analyze: 본문에서 URL을 가져와 분석을 위해 SQS 큐에 제출한다.
- GET /url/list: 프런트엔드가 시스템이 처리한 URL 목록을 가져오는 데 사용한다.
- GET /image/list 지정된 URL에 대해 처리한 일련의 이미지와 분석 결과를 반환한다.

### ◉ 비동기식 서비스
- 크롤러 서비스는 HTML 페이지에서 이미지를 추출한다.
- 분석 서비스는 AWS 레코그니션에 인터페이스를 제공하여 분석용 이미지를 제출하고 결과를 대조한다.

다운로드 메시지를 받으면 크롤러 서비스는 제공된 URL에서 HTML을 가져온다.
크롤러는 해당 HTML을 분석하고 페이지의 각 인라인 이미지 태그에서 소스 속성을 추출해 각 이미지를 다운로드한 뒤 S3 버킷에 저장한다.
크롤러는 모든 이미지를 다운로드하면 분석 SQS 큐에 아래와 같은 형식의 분석 메시지를 게시한다.
> {body: {action: "analyze", msg: {domain: "ai-as-a-service.s3-website-ap-northeast-2.amazonaws.com"}}}

분석 서비스는 이 메시지를 선택해, 이미지 인식 AI를 호출하여 다운로드한 모든 이미지에 대한 결과를 수집하고, 추후 프런트엔드에 표시할 수 있또록 버킷에 결과를 기록한다.

### ◉ 통신 서비스
시스템 내부에서는 메시지 파이프라인으로 SQS를 사용한다.
서비스 검색을 위해 라우트53 도메인 네임 시스템과 통신 프로토콜 HTTP, SQS 세 가지의 통신 서비스를 사용한다.
일반적으로 JSON 데이터 형식을 사용하여 당사자 간의 메시지를 인코딩한다.

### ◉ AI 서비스
AI 서비스를 아마존 레코그니션 한 가지만 사용한다.

#### ✦ 레코그니션
AWS에서 제공하는 이미지 및 동영상 분석 서비스이다. 기계 학습(ML) 기반으로 설계되어 다양한 시각적 콘텐츠를 자동으로 분석하고, 객체, 사람, 텍스트, 활동 등을 감지하는 기능을 제공한다.

### ◉ 데이터 서비스
데이터 서비스로 아마존 S3만 사용한다.

### ◉ 개발 지원과 운영 지원
서버리스 프레임워크를 주요 개발 지원 시스템으로 사용하며, 모든 로그 데이터는 클라우드워치를 사용하여 수집한다.

### ◉ 서버리스 프레임워크
서버리스(Serverless) 아키텍처를 쉽게 구현하고 관리할 수 있도록 도와주는 오픈소스 프레임워크이다.
이 프레임워크는 AWS Lambda, Azure Functions, Google Cloud Functions 등 다양한 서버리스 클라우드 서비스와 통합되며, 개발자가 코드를 작성하고 배포하는 과정을 간소화해준다.

Serverless Framework v4 CLI는 무료로 사용할 수 있지만, 몇 가지 제한 사항이 추가되었다.
연 매출이 $2M(200만 달러) 이하인 개발자나 조직은 여전히 무료로 사용할 수 있지만, 계정 등록(Account Registration) 또는 라이선스 키가 필요하다.
- https://www.serverless.com/

#### ✦ 설치
> npm install -g serverless

### ◉ 코드 다운로드
> git clone https://github.com/hanbit/ai-as-a-service.git

### ◉ 클라우드 리소스 설정
serverless.yml 파일을 사용하여 클라우드 리소스를 정의한다.
```yaml
service: resources
frameworkVersion: ">=4.0.0"
custom:
  bucket: ${env:CHAPTER2_BUCKET}
  crawlerqueue: Chap2CrawlerQueue
  analysisqueue: Chap2AnalysisQueue
  region: ${env:AWS_DEFAULT_REGION, 'ap-northeast-2'}
  accountid: ${env:AWS_ACCOUNT_ID}

provider:
  name: aws
  runtime: nodejs14.x
  stage: dev
  region: ${env:AWS_DEFAULT_REGION, 'ap-northeast-2'}

resources:
  Resources:
    WebAppS3Bucket:
      Type: AWS::S3::Bucket
      Properties:
        BucketName: ${self:custom.bucket}
        PublicAccessBlockConfiguration:
          BlockPublicAcls: false
          IgnorePublicAcls: false
          BlockPublicPolicy: false
          RestrictPublicBuckets: false
        WebsiteConfiguration:
          IndexDocument: index.html
          ErrorDocument: index.html
    WebAppS3BucketPolicy:
      Type: AWS::S3::BucketPolicy
      Properties:
        Bucket:
          Ref: WebAppS3Bucket
        PolicyDocument:
          Statement:
            - Sid: PublicReadGetObject
              Effect: Allow
              Principal: "*"
              Action:
                - s3:GetObject
              Resource: arn:aws:s3:::${self:custom.bucket}/*
    Chap2CrawlerQueue:
      Type: "AWS::SQS::Queue"
      Properties:
        QueueName: "${self:custom.crawlerqueue}"
    Chap2AnalysisQueue:
      Type: "AWS::SQS::Queue"
      Properties:
        QueueName: "${self:custom.analysisqueue}"


```

- service: 애플리케이션 또는 서비스의 이름을 정의
- frameworkVersion: 사용할 서버리스 프레임워크 버전을 정의
- custom: 사용자 정의 변수를 정의
    - 환경 변수(env)를 참조하거나 기본값을 설정.
- provider: 프레임워크에서 공급자에 특화된 설정을 정의한다. AWS 외에도 다른 여러 클라우드 플랫폼도 지원한다.
- functions: 서비스가 구현하는 함수 엔드포인트를 정의한다. 각 함수는 이벤트를 트리거하고, 이벤트에 대한 응답을 반환한다. (정의할 함수가 없으므로 예제에서는 생략)
- resources: 서비스에 필요한 클라우드 리소스를 정의한다. AWS CloudFormation 템플릿을 사용하여 리소스를 생성한다.
    - WebAppS3Bucket: S3 버킷을 생성한다.
        - BucketName: 버킷의 이름을 정의한다.
        - AccessControl: 버킷에 대한 액세스 권한을 정의한다.
        - WebsiteConfiguration: 정적 웹사이트 호스팅을 위한 설정한다.
            - IndexDocument: 인덱스 페이지를 정의한다.
            - ErrorDocument: 오류 페이지를 정의한다.
    - WebAppS3BucketPolicy: S3 버킷에 대한 정책을 정의한다.
        - Bucket: 정책을 적용할 버킷을 참조한다.
        - PolicyDocument: 버킷에 적용할 정책을 정의한다.
            - Statement: 정책의 규칙을 정의한다.
                - Sid: 정책 규칙의 식별자를 정의한다.
                - Effect: 규칙의 효과를 정의한다.
                - Principal: 규칙을 적용할 주체를 정의한다.
                - Action: 규칙이 허용할 액션을 정의한다.
                - Resource: 규칙이 적용될 리소스를 정의한다.
    - Chap2CrawlerQueue: 크롤러 큐를 생성한다.
        - QueueName: 큐의 이름을 정의한다.
    - Chap2AnalysisQueue: 분석 큐를 생성한다.

#### ✦ 정의된 리소스와 코드를 자동으로 생성 및 배포
> serverless deploy

#### ✦ CloudFormation
AWS 리소스를 코드로 정의하여, 인프라를 생성, 관리, 업데이트, 삭제할 수 있게 해주는 Infrastructure as Code(IaC) 서비스이다.

##### 작동 방식
1. CloudFormation 템플릿을 작성한다.
2. 템플릿을 사용하여 스택을 생성한다.
3. 스택을 통해 리소스를 생성한다.
4. 스택을 삭제하면 리소스도 함께 삭제된다.

##### 장점
- 코드로 인프라를 관리할 수 있다.
- 리소스를 생성, 수정, 삭제할 때 일관성을 유지할 수 있다.
- 리소스 간의 의존성을 관리할 수 있다.
- 리소스를 버전 관리할 수 있다.
- 리소스를 재사용할 수 있다.

##### serverless deploy와의 연동
서버리스 프레임워크는 CloudFormation을 사용하여 서버리스 애플리케이션을 배포한다.
serverless deploy 명령을 실행하면 <서비스명>-<스테이지> 형식으로 CloudFormation 스택을 생성하고, 서버리스 애플리케이션을 배포한다.

#### ✦ S3 버킷의 퍼블릭 정책 차단
AWS는 S3 퍼블릭 액세스 차단을 기본적으로 활성화하여 보안 위험을 줄이고 있다.
CloudFormation 템플릿을 사용하여 S3 버킷을 생성하면 퍼블릭 액세스 차단이 활성화되어 퍼블릭 액세스를 허용하는 버킷 정책을 추가하려고하면 오류가 발생한다.
PublicAccessBlockConfiguration 설정을 통해 퍼블릭 엑세스 차단을 비활성화할 수 있다.

### ◉ 비동기식 서비스 구현
#### ✦ 크롤러 서비스
크롤링할 대상 URL이 포함된 메시지가 큐에 배치되면 호출된다. 크롤러는 지정된 URL에서 HTML 페이지를 가져오고, 이미지 태그를 파싱한 다음 각 이미지를 차례대로 S3로 다운로드 한다.
모든 이미지를 다운로드하면 마지막으로 다음 프로세스를 위해 분석된 URL의 도메인 이름을 포함한 분석 메시지를 분석 큐에 포스트한다.

<img src="../../resources/images/크롤러 서비스.jpg" alt="크롤러 서비스" style="width: 100%; height: auto;" />

#### ✦ 크롤러 서비스를 위한 serverless.yml 파일
```yaml
service: crawler-service
frameworkVersion: ">=4.0.0"
custom:
  bucket: ${env:CHAPTER2_BUCKET}
  crawlerqueue: Chap2CrawlerQueue
  analysisqueue: Chap2AnalysisQueue
  region: ${env:AWS_DEFAULT_REGION, 'ap-northeast-2'}
  accountid: ${env:AWS_ACCOUNT_ID}

provider:
  name: aws
  runtime: nodejs20.x
  stage: dev
  region: ${env:AWS_DEFAULT_REGION, 'ap-northeast-2'}
  iamRoleStatements:
    - Effect: Allow
      Action:
        - s3:PutObject
      Resource: "arn:aws:s3:::${self:custom.bucket}/*"
    - Effect: Allow
      Action:
        - sqs:ListQueues
      Resource: "arn:aws:sqs:${self:provider.region}:*:*"
    - Effect: Allow
      Action:
        - sqs:ReceiveMessage
        - sqs:DeleteMessage
        - sqs:GetQueueUrl
      Resource: "arn:aws:sqs:*:*:${self:custom.crawlerqueue}"
    - Effect: Allow
      Action:
        - sqs:SendMessage
        - sqs:DeleteMessage
        - sqs:GetQueueUrl
      Resource: "arn:aws:sqs:*:*:${self:custom.analysisqueue}"

functions:
  crawlImages:
    handler: handler.crawlImages
    environment:
      BUCKET: ${self:custom.bucket}
      ANALYSIS_QUEUE: ${self:custom.analysisqueue}
      REGION: ${self:custom.region}
      ACCOUNTID: ${self:custom.accountid}
    events:
      - sqs:
          arn: "arn:aws:sqs:${self:provider.region}:${env:AWS_ACCOUNT_ID}:${self:custom.crawlerqueue}"
```

- iamRoleStatements: 함수가 사용할 IAM 역할을 정의한다.
    - s3:PutObject: S3 버킷에 객체를 업로드할 수 있는 권한을 부여한다.
    - sqs:ListQueues: SQS 큐 목록을 가져올 수 있는 권한을 부여한다.
    - sqs:ReceiveMessage: SQS 큐에서 메시지를 수신할 수 있는 권한을 부여한다.
    - sqs:DeleteMessage: SQS 큐에서 메시지를 삭제할 수 있는 권한을 부여한다.
    - sqs:GetQueueUrl: SQS 큐의 URL을 가져올 수 있는 권한을 부여한다.
    - sqs:SendMessage: SQS 큐에 메시지를 보낼 수 있는 권한을 부여한다.
    - sqs:DeleteMessage: SQS 큐에서 메시지를 삭제할 수 있는 권한을 부여한다.
    - sqs:GetQueueUrl: SQS 큐의 URL을 가져올 수 있는 권한을 부여한다.
- crawlImages: 크롤러 서비스의 핸들러 함수를 정의한다.
    - handler: 크롤러 서비스의 진입점을 정의한다.
    - environment: 환경 변수를 정의한다.
    - events: 크롤러 서비스를 트리거하는 이벤트를 정의한다.

#### ✦ handler.js 파일을 사용하여 크롤러 서비스를 구현한다.
```javascript
'use strict'

const request = require('request')
const urlParser = require('url')
const URLSearchParams = require('url').URLSearchParams
const shortid = require('shortid')
const asnc = require('async')
const AWS = require('aws-sdk')
const s3 = new AWS.S3()
const sqs = new AWS.SQS({region: process.env.REGION})
const images = require('./images')()

// 크롤링한 이미지의 상태 정보를 AWS S3에 JSON 파일로 저장하는 함수
function writeStatus (url, domain, results) {
  let parsed = urlParser.parse(url)
  parsed.hostname = domain
  parsed.host = domain
    
  // 상태 파일 생성
  const statFile = {
    url: urlParser.format(parsed),
    stat: 'downloaded',
    downloadResults: results
  }
  
  // S3에 상태 파일을 업로드
  return new Promise((resolve) => {
    s3.putObject({Bucket: process.env.BUCKET, Key: domain + '/status.json', Body: Buffer.from(JSON.stringify(statFile, null, 2), 'utf8')}, (err, data) => {
      resolve({stat: err || 'ok'})
    })
  })
}

// 도메인을 생성하는 함수
function createUniqueDomain (url) {
  const parsed = urlParser.parse(url)
  const sp = new URLSearchParams(parsed.search)
  let domain
    
  if (sp.get('q')) {
    // 도메인 앞에 쿼리 문자열 추가
    domain = sp.get('q') + '.' + parsed.hostname
  } else {
    // 도메인 앞에 랜덤 문자열 추가
    domain = shortid.generate() + '.' + parsed.hostname
  }
  domain = domain.replace(/ /g, '')
  return domain.toLowerCase()
}

// 크롤링 함수
function crawl (domain, url, context) {
  console.log('crawling: ' + url)
  return new Promise(resolve => {
    // HTTP 요청을 보내고 웹 페이지(HTML)를 가져온다. 
    request(url, (err, response, body) => {
      if (err || response.statusCode !== 200) { return resolve({statusCode: 500, body: err}) }
      // <img> 태그에서 이미지 URL을 추출하는 함수
      images.parseImageUrls(body, url).then(urls => {
        // 추출된 이미지 URL을 다운로드하는 함수
        images.fetchImages(urls, domain).then(results => {
          // 다운로드한 이미지 상태를 S3에 저장하는 함수
          writeStatus(url, domain, results).then(result => {
            resolve({statusCode: 200, body: JSON.stringify(result)})
          })
        })
      })
    })
  })
}

// 분석 큐에 메시지를 전송하는 함수
function queueAnalysis (domain, url, context) {
  let accountId = process.env.ACCOUNTID
    
  // AWS 계정 ID를 찾을 수 없는 경우, ARN에서 추출
  if (!accountId) {
    accountId = context.invokedFunctionArn.split(':')[4]
  }
  let queueUrl = `https://sqs.${process.env.REGION}.amazonaws.com/${accountId}/${process.env.ANALYSIS_QUEUE}`

  // 분석 큐에 보낼 메시지 생성
  let params = {
    MessageBody: JSON.stringify({action: 'analyze', msg: {domain: domain}}),
    QueueUrl: queueUrl
  }

  // SQS에 메시지 전송
  return new Promise(resolve => {
    sqs.sendMessage(params, (err, data) => {
      if (err) { console.log('QUEUE ERROR: ' + err); return resolve({statusCode: 500, body: err}) }
      console.log('queued analysis: ' + queueUrl)
      resolve({statusCode: 200, body: {queue: queueUrl, msgId: data.MessageId}})
    })
  })
}

/* 크롤러 서비스의 진입점
 * event: 처리 중인 현재 이벤트에 대한 정보를 제공한다. 객체가 SQS 큐에서 가져온 레코드 배열을 보유한다.
 * context: 호출에 대한 정보를 제공하기 위하여 AWS에서 사용하는 인수로, 사용 가능한 메모리, 실행 시간, 클라이언트 호출 컨텍스트 등의 정보를 담는다.
 * cb: 콜백 함수, 처리가 완료되면 핸들러가 결과와 함꼐 이 함수를 호출한다.
 */
module.exports.crawlImages = function (event, context, cb) {
    // 메시지를 기준으로 반복 처리
  asnc.eachSeries(event.Records, (record, asnCb) => {
    let { body } = record

    try {
      body = JSON.parse(body)
    } catch (exp) {
      return asnCb('message parse error: ' + record)
    }

    // 메시지 유형 확인 및 유효성 검사
    if (body.action === 'download' && body.msg && body.msg.url) {
      // 도메인 생성
      const udomain = createUniqueDomain(body.msg.url)
      // 크롤링 함수 호출
      crawl(udomain, body.msg.url, context).then(result => {
        // 분석 큐에 메시지를 전송하는 함수 호출
        queueAnalysis(udomain, body.msg.url, context).then(result => {
          asnCb(null, result)
        })
      })
    } else {
      asnCb('malformed message')
    }
  }, (err) => {
    if (err) { console.log(err) }
    cb()
  })
}
```

#### ✦ 크롤러 서비스 배포
> npm install
> serverless deploy

##### nodejs14.x 에러
AWS Lambda가 nodejs14.x 런타임을 더 이상 지원하지 않아 오류가 발생한다. (nodejs20.x로 변경) 

#### ✦ 크롤러 메시지 전송 테스트
- AWS 콘솔 > SQS > Chap2CrawlerQueue > 메시지 전송 및 수신 > 메시지 입력 후 전송
```json
// 메시지 내용
{
    "action": "download",
    "msg": {
        "url": "https://ai-as-a-service.s3-website-ap-northeast-2.amazonaws.com"
    }
}
```

#### ✦ 크롤러 서비스 로그 확인
- AWS 콘솔 > CloudWatch > 로그 그룹 > /aws/lambda/crawler-service-dev-crawlImages > 로그 스트림 > 로그 확인

#### ✦ 크롤러 서비스 결과 S3에서 확인
- AWS 콘솔 > S3 > 내 버킷 > [임의의-문자열].도메인 주소

5.웹 애플리케이션에 AI 인터페이스 추가하기
=========================
## ⭕ 3단계: STT 인터페이스 추가
<img src="../../resources/images/chapter_2.서비리스 AI 활용하기/트랜스크라이브 아키텍처.jpg" alt="트랜스크라이브 아키텍처.jpg" style="width: 100%; height: auto;" />

시스테브라우저를 사용하여 음성 오디오를 캡처하며 애플리파이 라이브러리를 사용하여 S3에 저장한다. S3에 오디오 파일이 업로드되면 노트서비스가 호출되고, 트랜스크라이브가 오디오를 텍스트로 변환하는 작업을 시작한다.
클라이언트는 주기적으로 노트 서비를 폴링하여 음성-텍스트 변환 완료 여부를 확인한다. 마지막으로 프런트엔드는 변환된 텍스트로 메모 필드를 채운다.

### ◉ 노트 서비스
```yaml
// note-service/serverless.yml
service: chapter4-note
frameworkVersion: ">=4.0.0"
plugins:
  - serverless-offline
  - serverless-domain-manager
  - serverless-dotenv-plugin
custom:
  dataBucket: ${env:CHAPTER4_DATA_BUCKET}
  region: ${env:AWS_DEFAULT_REGION, 'ap-northeast-2'}
  accountid: ${env:AWS_ACCOUNT_ID}
  poolArn: ${env:CHAPTER4_POOL_ARN}
  domain: ${env:CHAPTER4_DOMAIN}
  dotenv:
    path: ../.env
  customDomain:
    domainName: 'chapter4api.${self:custom.domain}'
    stage: ${self:provider.stage}
    basePath: noteapi
    certificateName: '*.${self:custom.domain}'
    certificateArn: ${env:CHAPTER4_DOMAIN_ARN}
    createRoute53Record: true
    endpointType: regional
  serverless-offline:
    port: 3001

provider:
  name: aws
  runtime: nodejs20.x
  stage: ${opt:stage, 'dev'}
  region: ${env:AWS_DEFAULT_REGION, 'ap-northeast-2'}
  iamRoleStatements:
    - Effect: Allow
      Action:
        - s3:PutObject
        - s3:GetObject
      Resource: "arn:aws:s3:::${self:custom.dataBucket}/*"
    - Effect: Allow
      Action:
        - transcribe:*
      Resource: "*"

functions:
  transcribe:
    handler: handler.transcribe
    events:
      - http:
          method: POST
          path: note
          cors: true
          authorizer:
            arn: '${self:custom.poolArn}'
  poll:
    handler: handler.poll
    events:
      - http:
          method: GET
          path: note/{id}
          cors: true
          authorizer:
            arn: '${self:custom.poolArn}'
```

- iamRoleStatements
  - action
    - transcribe:* : 트랜스크라이브 서비스에 대한 모든 권한을 부여한다.
- functions
  - transcribe
    - handler : handler.transcribe : 오디오 파일을 텍스트로 변환하는 람다 함수
- poll
  - handler : handler.poll : 변환 작업을 폴링하는 람다 함수

노트 서비스는 POST /note, GET /note/{id}의 두 가지 경로를 정의하고 각각 노트를 생성해서 가져온다.
코그니토 풀을 사용하여 노트 API에 대한 접근을 차단하고, noteapi의 다른 기본 경로와 함꼐 동일한 커스텀 도메인 구조를 사용한다.
핸들러 코드는 AWS SDK를 사용하여 다음 예시의 받아쓰기 작업을 생성한다.

```javascript
// note-service/handler.js
'use strict'

const request = require('request')
const AWS = require('aws-sdk')
var trans = new AWS.TranscribeService()

// 응답을 반환하는 함수
function respond (err, body, cb) {
  let statusCode = 200

  body = body || {}
  if (err) {
    body.stat = 'err'
    body.err = err
    if (err.statusCode) {
      statusCode = err.statusCode
    } else {
      statusCode = 500
    }
  } else {
    body.stat = 'ok'
  }

  const response = {
    headers: {
      'Content-Type': 'application/json',
      'Access-Control-Allow-Origin': '*',
      'Access-Control-Allow-Credentials': true,
      statusCode: statusCode
    },
    body: JSON.stringify(body)
  }

  cb(null, response)
}

// 오디오 파일을 텍스트로 변환하는 함수
module.exports.transcribe = (event, context, cb) => {
  const body = JSON.parse(event.body)

  const params = {
    LanguageCode: body.noteLang, // 오디오 파일 언어 설정
    Media: { MediaFileUri: body.noteUri }, // 오디오 파일 S3 경로
    MediaFormat: body.noteFormat, // 오디오 파일 포맷
    TranscriptionJobName: body.noteName, // 트랜스크립션 작업 이름
    MediaSampleRateHertz: body.noteSampleRate, // 오디오 파일 샘플링 속도
    Settings: {
      ChannelIdentification: false, // 여러 채널을 구분할 지 여부
      MaxSpeakerLabels: 4, // 최대 몇명까지 구분할 지
      ShowSpeakerLabels: true // 인원 구분 여부
    }
  }

  // 변환 작업을 시작하는 함수
  trans.startTranscriptionJob(params, (err, data) => {
    respond(err, data, cb)
  })
}

// 주기적으로 변환 작업 상태를 확인하는 함수
module.exports.poll = (event, context, cb) => {
  const params = { TranscriptionJobName: event.pathParameters.id }

  console.log(JSON.stringify(params))
  // 변환 작업 상태를 확인하는 함수
  trans.getTranscriptionJob(params, (err, data) => {
    // 오류가 발생하면 오류 메시지를 반환
    if (err) {
      console.log(err)
      return respond(err, {}, cb)
    }

    if (data && data.TranscriptionJob) {
      // 변환 작업이 완료되면 변환된 텍스트를 반환
      if (data.TranscriptionJob.TranscriptionJobStatus === 'COMPLETED') {
        // 변환된 텍스트 파일을 가져오는 함수
        request(data.TranscriptionJob.Transcript.TranscriptFileUri, (err, response, responseBody) => {
          let result

          if (!err && response && response.statusCode === 200) {
            // 응답 결과를 JSON 형태로 변환
            result = JSON.parse(responseBody)
          } else {
            // 오류가 발생하면 오류 메시지와 상태 코드를 반환
            result = {resultErr: err, resultResponse: response.statusCode}
          }

          // 변환 상태를 반환
          result.transcribeStatus = data.TranscriptionJob.TranscriptionJobStatus
          console.log(JSON.stringify(result))

          // 변환 결과를 반환
          respond(err, result, cb)
        })
      } else {
        console.log(JSON.stringify({transcribeStatus: data.TranscriptionJob.TranscriptionJobStatus}))
        // 변환이 완료되지 않은 경우 변환 상태만 반환
        respond(err, {transcribeStatus: data.TranscriptionJob.TranscriptionJobStatus}, cb)
      }
    } else {
      // 변환 작업이 존재하지 않을 경우 빈 객체 반환
      respond(err, {}, cb)
    }
  })
}
```

노트 서비스 핸들러는 작업을 시작하고 오디오 파일에 대한 링크를 전달하는 코드이다.
poll 함수는 주기적으로 변환 작업 상태를 확인하고, 작업이 완료되면 변환된 텍스트를 반환한다.

### ◉ 프런트엔드 업데이트
```javascript
// frontend/src/index.js
Amplify.configure({
  Auth: {
    region: process.env.TARGET_REGION,
    userPoolId: process.env.CHAPTER4_POOL_ID, // 사용자 풀 ID
    userPoolWebClientId: process.env.CHAPTER4_POOL_CLIENT_ID, // 사용자 풀 클라이언트 ID
    identityPoolId: process.env.CHAPTER4_IDPOOL, // identity 풀 ID
    mandatorySignIn: false, // 필수 로그인 여부
    oauth: oauth // oauth 설정 적용
  },
  // S3 설정
  Storage: {
    bucket: process.env.CHAPTER4_DATA_BUCKET, // 데이터 버킷
    region: process.env.TARGET_REGION, // 리전
    identityPoolId: process.env.CHAPTER4_IDPOOL, // identity 풀 ID
    level: 'public' // 데이터 접근 레벨
  }
})
```

이전에 추가한 index.js 파일에 앰플리파이 라이브러리를 사용하여 오디오를 업로드하기 위해 S3 설정을 추가한다.

```javascript
// frontend/src/note-view.js
'use strict'

import $ from 'jquery'

const view = { renderNote }
export { view }

// 노트 영역에 텍스트를 출력
function renderNote (text) {
  $('#todo-note').text(text)
}
```

```javascript
// frontend/src/note.js
/* globals alert:false */
'use strict'

import $ from 'jquery'
import {Storage} from 'aws-amplify'
import {AudioControl} from './audio/control'
import uuid from 'uuid/v1'
import {view} from './note-view'

const note = {activate, bindRecord}
export {note}

const API_ROOT = `https://chapter4api.${process.env.CHAPTER4_DOMAIN}/noteapi/note/`
const DATA_BUCKET_ROOT = `https://s3-${process.env.TARGET_REGION}.amazonaws.com/${process.env.CHAPTER4_DATA_BUCKET}/public/`

let auth
let ac
let itv

// 3초마다 폴링하여 결과를 출력
function pollNote (noteId) {
  let count = 0
  itv = setInterval(() => {
    auth.session().then(session => {
      $.ajax(API_ROOT + noteId, {
        type: 'GET',
        headers: {
          Authorization: session.idToken.jwtToken
        },
        success: function (body) {
          if (body.transcribeStatus === 'COMPLETED') {
            clearInterval(itv)
            // 오디오 변환 결과 출력
            view.renderNote(body.results.transcripts[0].transcript)
          } else if (body.transcribeStatus === 'FAILED') {
            clearInterval(itv)
            view.renderNote('FAILED')
          } else {
            count++
            let dots = ''
            for (let idx = 0; idx < count; idx++) {
              dots = dots + '.'
            }
            // 진행 중인 상태 출력
            view.renderNote('Still thinking' + dots)
          }
        }
      })
    }).catch(err => view.renderError(err))
  }, 3000)
}

// 녹음된 오디오 데이터를 서버로 전송
function submitNote (noteId, recordedSampleRate) {
  // 서버로 전송할 데이터
  const body = {
    noteLang: 'en-US',
    noteUri: DATA_BUCKET_ROOT + noteId + '.wav',
    noteFormat: 'wav',
    noteName: noteId,
    noteSampleRate: recordedSampleRate
  }

  // 세션을 획득
  auth.session().then(session => {
    $.ajax(API_ROOT, {
      data: JSON.stringify(body),
      contentType: 'application/json',
      type: 'POST',
      headers: {
        Authorization: session.idToken.jwtToken
      },
      success: function (body) {
        if (body.stat === 'ok') {
          // 노트 상태 폴링
          pollNote(noteId)
        } else {
          $('#error').html(body.err)
        }
      }
    })
  }).catch(err => view.renderError(err))
}

// 녹음 시작하는 함수
function startRecord () {
  // 녹음 시작 시 노트 영역에 Speak 출력
  view.renderNote('Speak')

  // 오디오 녹음을 제어하는 객체 생성
  ac = AudioControl({checkAudioSupport: false})

  // 오디오 지원 여부 확인
  ac.supportsAudio((supported) => {
    if (supported) {
      // 녹음 시작
      ac.startRecording()
    } else {
      alert('No audio support!')
    }
  })
}

// 녹음 중지하는 함수
function stopRecord () {
  // uuid 라이브러리를 통해 noteId 생성
  const noteId = uuid()

  // 녹음 중지 시 노트 영역에 Thinking 출력
  view.renderNote('Thinking')
  // 녹음 중지
  ac.stopRecording()

  // 녹음된 오디오 데이터를 WAV 파일로 변환
  ac.exportWAV((blob, recordedSampleRate) => {
    // WAV 파일을 S3에 업로드
    Storage.put(noteId + '.wav', blob)
            .then(result => {
              // 녹음된 오디오 데이터를 서버로 전송
              submitNote(noteId, recordedSampleRate)
            })
            .catch(err => {
              console.log(err)
            })
    // 오디오 객체 종료
    ac.close()
  })
}

// 녹음 버튼 이벤트 바인딩
function bindRecord () {
  $('#todo-note-start').unbind('click')
  $('#todo-note-start').on('click', e => {
    // 녹음 시작
    startRecord()
  })

  $('#todo-note-stop').unbind('click')
  $('#todo-note-stop').on('click', e => {
    // 녹음 중지
    stopRecord()
  })
}

function activate (authObj) {
  auth = authObj
}
```

```javascript
// frontend/src/control.js
'use strict'

import {AudioRecorder} from './recorder.js'

export {AudioControl}

// 오디오 녹음 및 재생을 제어하는 객체
function AudioControl (options) {
  let recorder
  let audioRecorder
  let checkAudioSupport
  let audioSupported
  let playbackSource
  let UNSUPPORTED = 'Audio is not supported.'
  options = options || {}
  // 오디오 지원 여부를 확인하는 옵션
  checkAudioSupport = options.checkAudioSupport !== false

  // 녹음 시작
  function startRecording (onSilence, visualizer, silenceDetectionConfig) {
    onSilence = onSilence || function () {}
    visualizer = visualizer || function () {}
    audioSupported = audioSupported !== false
    if (!audioSupported) {
      throw new Error(UNSUPPORTED)
    }

    // 오디오 녹음을 위한 객체 생성
    recorder = audioRecorder.createRecorder(silenceDetectionConfig)
    // 녹음 시작
    recorder.record(onSilence, visualizer)
  }

  // 녹음 중지
  function stopRecording () {
    audioSupported = audioSupported !== false
    if (!audioSupported) {
      throw new Error(UNSUPPORTED)
    }
    // 녹음 중지
    recorder.stop()
  }

  // 녹음된 오디오 데이터를 WAV 파일로 변환
  function exportWAV (callback, sampleRate) {
    audioSupported = audioSupported !== false
    if (!audioSupported) {
      throw new Error(UNSUPPORTED)
    }
    if (!(callback && typeof callback === 'function')) {
      throw new Error('You must pass a callback function to export.')
    }
    sampleRate = (typeof sampleRate !== 'undefined') ? sampleRate : 16000

    // WAV 파일로 변환
    recorder.exportWAV(callback, sampleRate)
    // 녹음된 데이터 초기화
    recorder.clear()
  }

  function playHtmlAudioElement (buffer, callback) {
    if (typeof buffer === 'undefined') {
      return
    }
    var myBlob = new Blob([buffer])
    var audio = document.createElement('audio')
    var objectUrl = window.URL.createObjectURL(myBlob)
    audio.src = objectUrl
    audio.addEventListener('ended', function () {
      audio.currentTime = 0
      if (typeof callback === 'function') {
        callback()
      }
    })
    audio.play()
  }

  function play (buffer, callback) {
    if (typeof buffer === 'undefined') {
      return
    }
    var myBlob = new Blob([buffer])
    // We'll use a FileReader to create and ArrayBuffer out of the audio response.
    var fileReader = new FileReader()
    fileReader.onload = function() {
      // Once we have an ArrayBuffer we can create our BufferSource and decode the result as an AudioBuffer.
      playbackSource = audioRecorder.audioContext().createBufferSource()
      audioRecorder.audioContext().decodeAudioData(this.result, function(buf) {
        // Set the source buffer as our new AudioBuffer.
        playbackSource.buffer = buf
        // Set the destination (the actual audio-rendering device--your device's speakers).
        playbackSource.connect(audioRecorder.audioContext().destination)
        // Add an "on ended" callback.
        playbackSource.onended = function(event) {
          if (typeof callback === 'function') {
            callback()
          }
        }
        // Start the playback.
        playbackSource.start(0)
      })
    }
    fileReader.readAsArrayBuffer(myBlob)
  }

  function stop () {
    if (typeof playbackSource === 'undefined') {
      return
    }
    playbackSource.stop()
  }

  function clear () {
    recorder.clear()
  }

  // 오디오 지원 여부 확인
  function supportsAudio (callback) {
    callback = callback || function () { }
    // 브라우저에서 오디오 지원 여부 확인
    if (navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {
      // 오디오 녹음을 위한 객체 생성
      audioRecorder = AudioRecorder()
      // 브라우저에서 오디오 권한을 요청하고 스트림을 생성
      audioRecorder.requestDevice()
              .then((stream) => {
                audioSupported = true
                callback(audioSupported)
              })
              .catch(() => {
                audioSupported = false
                callback(audioSupported)
              })
    } else {
      audioSupported = false
      callback(audioSupported)
    }
  }

  // 오디오 녹음 객체 종료
  function close () {
    audioRecorder.close()
  }

  if (checkAudioSupport) {
    supportsAudio()
  }

  return {
    startRecording,
    stopRecording,
    exportWAV,
    play,
    stop,
    clear,
    playHtmlAudioElement,
    supportsAudio,
    close
  }
}
```

```javascript
// frontend/src/recorder.js
/* globals AudioContext:false Worker:false */
'use strict'

export {AudioRecorder}

// 오디오 녹음을 제어하는 객체
function Recorder (source, silenceDetectionConfig) {
  // 워커 객체 생성
  let worker = new Worker('./worker.js')

  silenceDetectionConfig = silenceDetectionConfig || {}
  silenceDetectionConfig.time = silenceDetectionConfig.hasOwnProperty('time') ? silenceDetectionConfig.time : 1500 // 무음 시간
  silenceDetectionConfig.amplitude = silenceDetectionConfig.hasOwnProperty('amplitude') ? silenceDetectionConfig.amplitude : 0.2 // 무음 진폭

  let recording = false
  let currCallback
  let start
  let silenceCallback // 무음 콜백 함수
  let visualizationCallback // 시각화 콜백 함수
  let node = source.context.createScriptProcessor(4096, 1, 1) // 오디오 녹음을 위한 노드 생성

  // 워커로부터 메시지를 받아 처리
  worker.onmessage = function (message) {
    let blob = message.data
    currCallback(blob, source.context.sampleRate)
  }

  // 워커 초기화
  worker.postMessage({
    command: 'init',
    config: {
      sampleRate: source.context.sampleRate
    }
  })

  // 녹음 시작
  function record (onSilence, visualizer) {
    silenceCallback = onSilence // 무음 콜백 함수
    visualizationCallback = visualizer // 시각화 콜백 함수
    start = Date.now() // 녹음 시작 시간
    recording = true // 녹음 상태로 변경
  }

  // 녹음 중지
  function stop () {
    recording = false
  }

  // 녹음된 데이터 초기화
  function clear () {
    stop()
    worker.postMessage({command: 'clear'})
  }

  // 녹음된 오디오 데이터를 WAV 파일로 변환
  function exportWAV (callback, sampleRate) {
    currCallback = callback
    worker.postMessage({
      command: 'export',
      sampleRate: sampleRate
    })
  }

  // 실시간 오디오 감지와 무음 감지 콜백 처리
  function analyse () {
    analyser.fftSize = 2048
    let bufferLength = analyser.fftSize
    let dataArray = new Uint8Array(bufferLength)
    let amplitude = silenceDetectionConfig.amplitude
    let time = silenceDetectionConfig.time

    analyser.getByteTimeDomainData(dataArray)

    if (typeof visualizationCallback === 'function') {
      visualizationCallback(dataArray, bufferLength)
    }

    for (let i = 0; i < bufferLength; i++) {
      let currValueTime = (dataArray[i] / 128) - 1.0
      if (currValueTime > amplitude || currValueTime < (-1 * amplitude)) {
        start = Date.now()
      }
    }
    let newtime = Date.now()
    let elapsedTime = newtime - start
    if (elapsedTime > time) {
      silenceCallback()
    }
  }

  // 녹음 객체 종료
  function close () {
    worker.postMessage({command: 'close'})
  }


  // 노드에 오디오 처리 이벤트 추가 (녹음을 할 때 마다 자동으로 호출)
  node.onaudioprocess = function (audioProcessingEvent) {
    if (!recording) {
      return
    }

    // 녹음 중인 오디오 데이터를 워커로 전달
    worker.postMessage({
      command: 'record',
      buffer: [ audioProcessingEvent.inputBuffer.getChannelData(0) ]
    })

    // 녹음 중인 오디오 데이터를 분석
    analyse()
  }

  // 오디오 분석을 위한 객체 생성
  let analyser = source.context.createAnalyser()
  analyser.minDecibels = -90
  analyser.maxDecibels = -10
  analyser.smoothingTimeConstant = 0.85

  // 노드에 오디오 분석 객체 연결
  source.connect(analyser)
  // 노드에 오디오 녹음 객체 연결
  analyser.connect(node)
  // 노드에 오디오 출력 연결
  node.connect(source.context.destination)

  return {
    record,
    stop,
    clear,
    close,
    exportWAV
  }
}


// 오디오 녹음을 위한 객체
function AudioRecorder () {
  let audioCtx
  let audioStream
  let rec

  // 브라우저에서 오디오 권한을 요청하고 스트림을 생성
  function requestDevice () {
    // 오디오 컨텍스트가 없으면 생성
    if (typeof audioCtx === 'undefined') {
      window.AudioContext = window.AudioContext || window.webkitAudioContext
      audioCtx = new AudioContext() // 브라우저에서 제공되는 표준 오디오 API
    }

    // 브라우저에 오디오 권한을 요청하고 스트림을 생성하여 저장
    return navigator.mediaDevices.getUserMedia({audio: true}).then(function (stream) {
      audioStream = stream
    })
  }

  // 오디오 녹음을 제어하는 객체 생성
  function createRecorder (silenceDetectionConfig) {
    // 오디오 스트림을 이용하여 녹음 객체 생성
    rec = Recorder(audioCtx.createMediaStreamSource(audioStream), silenceDetectionConfig)
    return rec
  }


  function audioContext () {
    return audioCtx
  }

  // 오디오 녹음 객체 종료
  function close () {
    rec.close() // 녹음 객체 종료
    audioCtx.close() // 오디오 컨텍스트 종료
  }


  return {
    requestDevice,
    createRecorder,
    audioContext,
    close
  }
}
```

```javascript
// frontend/src/worker.js
/* globals Blob:false postMessage:false close:false */
'use strict'

let recLength = 0
let recBuffer = []
let recordSampleRate


self.onmessage = function (e) {
  switch (e.data.command) {
    case 'init':
      init(e.data.config)
      break
    case 'record':
      record(e.data.buffer)
      break
    case 'export':
      exportBuffer(e.data.samplerate)
      break
    case 'clear':
      clear()
      break
    case 'close':
      close()
      break
  }
}

// 녹음 초기화
function init (config) {
  recordSampleRate = config.sampleRate
}

// 녹음 데이터를 버퍼에 저장
function record (inputBuffer) {
  recBuffer.push(inputBuffer[0])
  recLength += inputBuffer[0].length
}

// 녹음된 데이터를 WAV 파일로 변환
function exportBuffer (exportSampleRate) {
  let mergedBuffers = mergeBuffers(recBuffer, recLength) // 버퍼를 합침
  let downsampledBuffer = downsampleBuffer(mergedBuffers, exportSampleRate) // 샘플레이트를 줄임
  let encodedWav = encodeWAV(downsampledBuffer) // WAV 파일로 인코딩
  let audioBlob = new Blob([encodedWav], {type: 'application/octet-stream'}) // 블롭 객체 생성
  postMessage(audioBlob) // 블롭 객체를 메인 스레드로 전송
}

// 버퍼 초기화
function clear () {
  recLength = 0
  recBuffer = []
}

// 녹음된 오디오 데이터를 지정한 샘플레이트로 줄이는 함수
function downsampleBuffer (buffer, exportSampleRate) {
  if (typeof exportSampleRate === 'undefined' || exportSampleRate === recordSampleRate) {
    return buffer
  }
  let sampleRateRatio = recordSampleRate / exportSampleRate
  let newLength = Math.round(buffer.length / sampleRateRatio)
  let result = new Float32Array(newLength)
  let offsetResult = 0
  let offsetBuffer = 0
  while (offsetResult < result.length) {
    let nextOffsetBuffer = Math.round((offsetResult + 1) * sampleRateRatio)
    let accum = 0
    let count = 0
    for (let i = offsetBuffer; i < nextOffsetBuffer && i < buffer.length; i++) {
      accum += buffer[i]
      count++
    }
    result[offsetResult] = accum / count
    offsetResult++
    offsetBuffer = nextOffsetBuffer
  }
  return result
}

// 녹음된 오디오 버퍼를 하나로 합치는 함수
function mergeBuffers (bufferArray, recLength) {
  let result = new Float32Array(recLength)
  let offset = 0
  for (let i = 0; i < bufferArray.length; i++) {
    result.set(bufferArray[i], offset)
    offset += bufferArray[i].length
  }
  return result
}

// 16비트 PCM으로 변환
function floatTo16BitPCM (output, offset, input) {
  for (let i = 0; i < input.length; i++, offset += 2) {
    let s = Math.max(-1, Math.min(1, input[i]))
    output.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7FFF, true)
  }
}

// 문자열을 바이너리 데이터로 변환 (WAV 파일은 바이너리 파일 포맷)
function writeString (view, offset, string) {
  for (let i = 0; i < string.length; i++) {
    view.setUint8(offset + i, string.charCodeAt(i))
  }
}

// WAV 파일로 인코딩
function encodeWAV (samples) {
  let buffer = new ArrayBuffer(44 + samples.length * 2)
  let view = new DataView(buffer)

  writeString(view, 0, 'RIFF')
  view.setUint32(4, 32 + samples.length * 2, true)
  writeString(view, 8, 'WAVE')
  writeString(view, 12, 'fmt ')
  view.setUint32(16, 16, true)
  view.setUint16(20, 1, true)
  view.setUint16(22, 1, true)
  view.setUint32(24, recordSampleRate, true)
  view.setUint32(28, recordSampleRate * 2, true)
  view.setUint16(32, 2, true)
  view.setUint16(34, 16, true)
  writeString(view, 36, 'data')
  view.setUint32(40, samples.length * 2, true)
  floatTo16BitPCM(view, 44, samples)

  return view
}
```

#### ✦ 프론트엔드 코드 설명
- note.js: 사용자의 녹음 시작/중지 버튼과 연동되며, 전체 오디오 녹음 → 저장 → 전사 결과 출력까지 담당
- control.js: 오디오 녹음/재생을 제어하는 중간 인터페이스 (AudioControl)
- recorder.js: 녹음 + 실시간 분석 + 무음 감지 + 워커 통신 처리
- worker.js: 녹음된 오디오 데이터를 백그라운드에서 처리하는 Web Worker

#### ✦ 프론트엔드 음성 녹음 동작 플로우
1. 사용자가 녹음 시작 버튼을 클릭하면 startRecord() 함수가 호출된다. 
2. startRecord() 함수는 오디오 녹음을 위한 AudioControl 객체를 생성하고, AudioRecorder를 통해 마이크 권한을 요청한 뒤 녹음을 시작한다. 
3. 내부적으로 Recorder 객체가 생성되며, 오디오 스트림을 ScriptProcessorNode로 실시간 수집하고, 동시에 Web Worker(worker.js)도 함께 초기화된다. 
4. 사용자가 녹음 중지 버튼을 클릭하면 stopRecord() 함수가 호출된다. 
5. stopRecord() 함수는 녹음을 중단하고, exportWAV() 함수를 통해 녹음된 오디오 데이터를 Web Worker에 전달하여 WAV 포맷으로 변환 요청을 보낸다. 
6. Web Worker는 다음 단계를 수행한다 
   1. 녹음된 오디오 버퍼들을 하나로 병합한다 (mergeBuffers)
   2. 샘플레이트를 downsampling 한다 (downsampleBuffer)
   3. 16비트 PCM WAV 포맷으로 인코딩 한다 (encodeWAV)
   4. 최종적으로 .wav 파일 Blob을 생성하고, postMessage()로 메인 스레드에 전달한다 
7. 메인 스레드는 전달받은 WAV Blob을 AWS S3에 업로드한다 (Storage.put() 사용)
8. 업로드가 완료되면 submitNote() 함수가 호출되어, S3 경로와 메타 정보를 포함한 데이터를 서버로 전송한다. 
9. 서버에서는 오디오 데이터를 받아 음성 인식 작업(STT)을 수행하고, 작업 상태를 프론트엔드가 주기적으로 pollNote()를 통해 폴링한다. 
10. 음성 인식 작업이 완료되면, 서버는 전사된 텍스트를 응답으로 보내고, 프론트엔드는 이를 받아 노트 영역에 출력한다 (view.renderNote()). 
11. 음성 인식이 실패하면, "FAILED" 메시지를 출력한다. 
12. 음성 인식이 아직 진행 중이면, "Still thinking..."과 점을 점점 추가해 사용자에게 진행 상태를 알려준다.

#### ❗️AudioContext란?
웹 브라우저에서 제공하는 API로, 오디오 데이터를 생성하고 처리하는 데 사용된다.

```html
<!DOCTYPE html>
<html>
<head>
  <title>AudioContext 예제</title>
</head>
<body>
  <button id="play">비프음 재생</button>

  <script>
    document.getElementById('play').addEventListener('click', () => {
      const audioCtx = new (window.AudioContext || window.webkitAudioContext)(); // 오디오 컨텍스트 생성

      const oscillator = audioCtx.createOscillator(); // 사운드 생성기
      oscillator.type = 'sine'; // 사운드 파형: sine, square, triangle, sawtooth
      oscillator.frequency.value = 440; // 주파수(Hz) - 440Hz는 라(A4)음

      oscillator.connect(audioCtx.destination); // 스피커로 연결
      oscillator.start(); // 재생 시작
      oscillator.stop(audioCtx.currentTime + 1); // 1초 뒤 정지
    });
  </script>
</body>
</html>
```

#### ❗️Web Worker란?
웹 브라우저에서 실행되는 스크립트로, 메인 스레드와 별도로 동작한다. 메인 스레드는 사용자 인터페이스를 처리하고, 웹 워커는 백그라운드 작업(별도 스레드)을 처리한다.

```html
<!DOCTYPE html>
<html>
<head>
  <title>Web Worker 예제</title>
</head>
<body>
  <h2>Web Worker 데모</h2>
  <button id="start">1부터 1억까지 더하기</button>
  <p id="result">결과: </p>

  <script>
    const worker = new Worker('worker.js') // 워커 생성

    document.getElementById('start').addEventListener('click', () => {
      document.getElementById('result').textContent = '계산 중...'
      worker.postMessage('start') // 워커에 작업 시작 요청
    })

    worker.onmessage = function (e) {
      document.getElementById('result').textContent = '결과: ' + e.data
    }
  </script>
</body>
</html>
```

### ◉ STT 인터페이스 추가 작업 배포 및 테스트
step-3-note-service/note-service 디렉토리 이동하여 실행
> npm install
> serverless deploy

step-3-note-service/frontend 디렉토리 이동하여 실행
> source ../.env
> npm install
> npm run build
> aws s3 sync dist/ s3://$CHAPTER4_BUCKET
